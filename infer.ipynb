{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REC8UsP7mr0-"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oumckS9eRAUb"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/bofenghuang/vigogne.git\n",
        "%cd vigogne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTSBldIT7Y96"
      },
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bhuang/anaconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 117\n",
            "CUDA SETUP: Loading binary /home/bhuang/anaconda3/envs/nlp/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bhuang/anaconda3/envs/nlp/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/bhuang/anaconda3/envs/nlp did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, LlamaTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n",
            "Loading checkpoint shards: 100%|██████████| 33/33 [00:39<00:00,  1.20s/it]\n"
          ]
        }
      ],
      "source": [
        "base_model_name_or_path = \"decapoda-research/llama-7b-hf\"\n",
        "lora_model_name_or_path = \"bofenghuang/vigogne-lora-7b\"\n",
        "\n",
        "# base_model_name_or_path = \"decapoda-research/llama-30b-hf\"\n",
        "# lora_model_name_or_path = \"bofenghuang/vigogne-lora-30b\"\n",
        "\n",
        "# base_model_name_or_path = \"bigscience/bloom-7b1\"\n",
        "# lora_model_name_or_path = \"bofenghuang/vigogne-lora-bloom-7b1\"\n",
        "\n",
        "load_8bit = False\n",
        "\n",
        "tokenizer_class = LlamaTokenizer if \"llama\" in base_model_name_or_path else AutoTokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(base_model_name_or_path)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if device == \"cuda\":\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name_or_path,\n",
        "        load_in_8bit=load_8bit,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        lora_model_name_or_path,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "elif device == \"mps\":\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name_or_path,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        lora_model_name_or_path,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name_or_path, device_map={\"\": device}, low_cpu_mem_usage=True\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        lora_model_name_or_path,\n",
        "        device_map={\"\": device},\n",
        "    )\n",
        "\n",
        "if not load_8bit:\n",
        "    model.half()  # seems to fix bugs for some users.\n",
        "\n",
        "model.eval()\n",
        "if torch.__version__ >= \"2\":\n",
        "    model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Ci-dessous se trouve une instruction qui décrit une tâche, associée à une entrée qui fournit un contexte supplémentaire. Écrivez une réponse qui complète correctement la demande.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Entrée:\\n{input}\\n\\n### Réponse:\\n\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Ci-dessous se trouve une instruction qui décrit une tâche. Écrivez une réponse qui complète correctement la demande.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Réponse:\\n\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    return (\n",
        "        PROMPT_DICT[\"prompt_input\"].format_map({\"instruction\": instruction, \"input\": input})\n",
        "        if input is not None\n",
        "        else PROMPT_DICT[\"prompt_no_input\"].format_map({\"instruction\": instruction})\n",
        "    )\n",
        "\n",
        "\n",
        "def instruct(\n",
        "    instruction,\n",
        "    input=None,\n",
        "    temperature=0.1,\n",
        "    no_repeat_ngram_size=3,\n",
        "    max_new_tokens=512,\n",
        "    **kwargs,\n",
        "):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    tokenized_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        **kwargs,\n",
        "    )\n",
        "    with torch.inference_mode():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            # output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "    return output.split(\"### Réponse:\")[1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bAl6_6PrizMb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Le théorême central limites affirme que la loi de la moyenne des valeurs de variables aléatoires indépendantes est la loi normale. Cela signifie que la distribution des valeur de la somme de plusieurs variables al équivalentes est la même que la normale, et que la variance de la distribution de la valeur moyenne de plusieurs de ces variables est la variance du résultat de la loi des grands nombres.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# instruct\n",
        "instruct(\"Expliquer le théorème central limite.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W8hRrkPcO3JR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Les papillons extraient des nutriments qui sont ensuite convertis en papillones. Les gens ont extrait des milliards de nugget de compréhension et GPT4 est la butterlfly de l'humanité.\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# instruct + input\n",
        "instruct(\n",
        "    \"Traduisez le texte suivant en français.\",\n",
        "    input=\"Caterpillars extract nutrients which are then converted into butterflies. People have extracted billions of nuggets of understanding and GPT-4 is humanity's butterfly.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.0 ('nlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f99034d7e2e0fcf8bb579b4deb12a7e98f13c7bc298256d783a54f6812e2c7dd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
