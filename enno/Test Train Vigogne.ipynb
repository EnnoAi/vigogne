{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPU2lQv2PoEzt89wMyofgb9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Prevent Google Colab from disconnecting?"],"metadata":{"id":"Q6PrW4hnZ720"}},{"cell_type":"markdown","source":["\n","\n","```\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\");\n","    document.querySelector(\"colab-connect-button\").click()\n","}\n","setInterval(ClickConnect, 97135)\n","```\n","\n"],"metadata":{"id":"gOBmWRqcZ4v1"}},{"cell_type":"markdown","source":["# Text learning model"],"metadata":{"id":"3nzESVNaF3TK"}},{"cell_type":"markdown","source":["JSON:\n","\n","```\n","{\"instruction\": \"Chez qui \\u00eates-vous d\\u00e9j\\u00e0 pr\\u00e9sents ?\", \"input\": \"\", \"output\": \"Pour des raisons de confidentialit\\u00e9s, nous ne divulguons pas ces informations.\"}\n","{\"instruction\": \"Combien co\\u00fbte EnnoData ?\", \"input\": \"\", \"output\": \"Sous r\\u00e9serve d'\\u00e9ligibilit\\u00e9 de votre entreprise, EnnoData est gratuit pendant 4 mois. Par la suite, le forfait mensuel co\\u00fbte 750 \\u20ac la premi\\u00e8re ann\\u00e9e.\"}\n","```\n","\n","After prompt build:\n","\n","\n","```\n","input_ids = \"\"\"<s> ### System:\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nChez qui êtes-vous déjà présents ?\\n\\n### Response:\\nPour des raisons de confidentialités, nous ne divulguons pas ces informations.</s>\"\"\"\n","\n","labels = \"\"\"Pour des raisons de confidentialités, nous ne divulguons pas ces informations.</s>\"\"\"\n","```\n","\n","\n","\n","```\n","input_tokens = [tokenizer.bos_token] + prompt_tokens + completion_tokens + [tokenizer.eos_token]\n","label_tokens = [tokenizer.bos_token] + [-100] * len(prompt_tokens) + completion_tokens + [tokenizer.eos_token]\n","```\n"],"metadata":{"id":"JaHW4BGrGBKB"}},{"cell_type":"markdown","source":["# Clone Vigogne"],"metadata":{"id":"tiUJhjWXwIDw"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQsoBbditPWZ","executionInfo":{"status":"ok","timestamp":1692995076331,"user_tz":-120,"elapsed":5924,"user":{"displayName":"Prudence Vol'Jin","userId":"11908822241771581805"}},"outputId":"e131bd59-b0ed-4ba0-c5a8-394de310e46c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'vigogne'...\n","remote: Enumerating objects: 827, done.\u001b[K\n","remote: Counting objects: 100% (340/340), done.\u001b[K\n","remote: Compressing objects: 100% (171/171), done.\u001b[K\n","remote: Total 827 (delta 205), reused 287 (delta 167), pack-reused 487\u001b[K\n","Receiving objects: 100% (827/827), 66.01 MiB | 23.08 MiB/s, done.\n","Resolving deltas: 100% (456/456), done.\n","Updating files: 100% (97/97), done.\n"]}],"source":["!git clone https://github.com/bofenghuang/vigogne.git"]},{"cell_type":"markdown","source":["# Install requirements"],"metadata":{"id":"YjUrqS0KwLRL"}},{"cell_type":"code","source":["!pip install deepspeed ninja accelerate\n","!pip install --no-build-isolation flash-attn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NlHmsMFltoPS","executionInfo":{"status":"ok","timestamp":1692995167820,"user_tz":-120,"elapsed":37714,"user":{"displayName":"Prudence Vol'Jin","userId":"11908822241771581805"}},"outputId":"3dfb25dc-0c9b-426f-fcf2-9cafff97bea3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deepspeed\n","  Downloading deepspeed-0.10.1.tar.gz (851 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/851.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/851.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m573.4/851.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.5/851.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ninja\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate\n","  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hjson (from deepspeed)\n","  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n","Collecting pydantic<2.0.0 (from deepspeed)\n","  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.0.1+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->deepspeed) (4.7.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepspeed) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepspeed) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.10.1-py3-none-any.whl size=891694 sha256=6f703f2a0276d78fbf729a9fa268ee36818aa36856a0f4f79cc548516149c94b\n","  Stored in directory: /root/.cache/pip/wheels/fd/5d/48/246fc22e6f69aa948d8f8542f6af89b121dd273bf2c754c049\n","Successfully built deepspeed\n","Installing collected packages: ninja, hjson, pydantic, deepspeed, accelerate\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.2.1\n","    Uninstalling pydantic-2.2.1:\n","      Successfully uninstalled pydantic-2.2.1\n","Successfully installed accelerate-0.22.0 deepspeed-0.10.1 hjson-3.1.0 ninja-1.11.1 pydantic-1.10.12\n","Collecting flash-attn\n","  Downloading flash_attn-2.1.0.tar.gz (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.0.1+cu118)\n","Collecting einops (from flash-attn)\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.1)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n","Building wheels for collected packages: flash-attn\n","  Building wheel for flash-attn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flash-attn: filename=flash_attn-2.1.0-cp310-cp310-linux_x86_64.whl size=128550871 sha256=2d81cefb7b3e0d4f01eb08238bf8a671679b55ff9c036e5b4c9c882a7913e9c3\n","  Stored in directory: /root/.cache/pip/wheels/34/6d/ad/c83a7ba7f7573219c64676caa306e66c227d32a877e13ba6f4\n","Successfully built flash-attn\n","Installing collected packages: einops, flash-attn\n","Successfully installed einops-0.6.1 flash-attn-2.1.0\n"]}]},{"cell_type":"markdown","source":["# Build Vigogne"],"metadata":{"id":"fyerfx7_wORp"}},{"cell_type":"code","source":["!cd /content/vigogne && pip install ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_vAObcit8Pj","executionInfo":{"status":"ok","timestamp":1692995284161,"user_tz":-120,"elapsed":100196,"user":{"displayName":"Prudence Vol'Jin","userId":"11908822241771581805"}},"outputId":"435d4852-5351-4878-bdeb-e92f5e702056"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing /content/vigogne\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate@ git+https://github.com/huggingface/accelerate.git (from vigogne==0.2.0)\n","  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-install-swnhy9pq/accelerate_543d726229dc48bc8e4ad3f1c72710c7\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-swnhy9pq/accelerate_543d726229dc48bc8e4ad3f1c72710c7\n","  Resolved https://github.com/huggingface/accelerate.git to commit e2ae254008061b3e53fc1c97f88d65743a857e75\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting peft@ git+https://github.com/huggingface/peft.git (from vigogne==0.2.0)\n","  Cloning https://github.com/huggingface/peft.git to /tmp/pip-install-swnhy9pq/peft_0864bf3eaf5340d182a03b0141e47a6d\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-install-swnhy9pq/peft_0864bf3eaf5340d182a03b0141e47a6d\n","  Resolved https://github.com/huggingface/peft.git to commit 8c17d556a8fe9522e10d73d7bd3fad46a6ecae14\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting transformers@ git+https://github.com/huggingface/transformers.git (from vigogne==0.2.0)\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-swnhy9pq/transformers_26df702472374c1d82509c161c1a3744\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-swnhy9pq/transformers_26df702472374c1d82509c161c1a3744\n","  Resolved https://github.com/huggingface/transformers.git to commit 960807f62e53676723ab8281019219864ef3db4d\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from vigogne==0.2.0) (1.4.4)\n","Collecting bitsandbytes (from vigogne==0.2.0)\n","  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets (from vigogne==0.2.0)\n","  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vigogne==0.2.0) (0.6.1)\n","Collecting fire (from vigogne==0.2.0)\n","  Downloading fire-0.5.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting loralib (from vigogne==0.2.0)\n","  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from vigogne==0.2.0) (3.8.1)\n","Collecting gradio (from vigogne==0.2.0)\n","  Downloading gradio-3.41.2-py3-none-any.whl (20.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai (from vigogne==0.2.0)\n","  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from vigogne==0.2.0) (1.10.12)\n","Collecting rouge-score (from vigogne==0.2.0)\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sentencepiece (from vigogne==0.2.0)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vigogne==0.2.0) (1.10.1)\n","Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from vigogne==0.2.0) (8.2.3)\n","Collecting tensorboardX (from vigogne==0.2.0)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken (from vigogne==0.2.0)\n","  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vigogne==0.2.0) (2.0.1+cu118)\n","Collecting wandb (from vigogne==0.2.0)\n","  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate@ git+https://github.com/huggingface/accelerate.git->vigogne==0.2.0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate@ git+https://github.com/huggingface/accelerate.git->vigogne==0.2.0) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate@ git+https://github.com/huggingface/accelerate.git->vigogne==0.2.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate@ git+https://github.com/huggingface/accelerate.git->vigogne==0.2.0) (6.0.1)\n","Collecting huggingface-hub (from accelerate@ git+https://github.com/huggingface/accelerate.git->vigogne==0.2.0)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->vigogne==0.2.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->vigogne==0.2.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vigogne==0.2.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vigogne==0.2.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->vigogne==0.2.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->vigogne==0.2.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->vigogne==0.2.0) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->vigogne==0.2.0) (16.0.6)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->vigogne==0.2.0) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets->vigogne==0.2.0)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->vigogne==0.2.0) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->vigogne==0.2.0) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets->vigogne==0.2.0) (4.66.1)\n","Collecting xxhash (from datasets->vigogne==0.2.0)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets->vigogne==0.2.0)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->vigogne==0.2.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->vigogne==0.2.0) (3.8.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->vigogne==0.2.0) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->vigogne==0.2.0) (2.3.0)\n","Collecting aiofiles<24.0,>=22.0 (from gradio->vigogne==0.2.0)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->vigogne==0.2.0) (4.2.2)\n","Collecting fastapi (from gradio->vigogne==0.2.0)\n","  Downloading fastapi-0.102.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio->vigogne==0.2.0)\n","  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.5.0 (from gradio->vigogne==0.2.0)\n","  Downloading gradio_client-0.5.0-py3-none-any.whl (298 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from gradio->vigogne==0.2.0)\n","  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->vigogne==0.2.0) (6.0.1)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->vigogne==0.2.0) (2.1.3)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->vigogne==0.2.0) (3.7.1)\n","Collecting orjson~=3.0 (from gradio->vigogne==0.2.0)\n","  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->vigogne==0.2.0) (9.4.0)\n","Collecting pydub (from gradio->vigogne==0.2.0)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart (from gradio->vigogne==0.2.0)\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio->vigogne==0.2.0)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting uvicorn>=0.14.0 (from gradio->vigogne==0.2.0)\n","  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio->vigogne==0.2.0)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->vigogne==0.2.0) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->vigogne==0.2.0) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->vigogne==0.2.0) (2023.6.3)\n","Collecting safetensors (from peft@ git+https://github.com/huggingface/peft.git->vigogne==0.2.0)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->vigogne==0.2.0) (1.4.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->vigogne==0.2.0) (3.20.3)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers@ git+https://github.com/huggingface/transformers.git->vigogne==0.2.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->vigogne==0.2.0)\n","  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->vigogne==0.2.0)\n","  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->vigogne==0.2.0)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools (from wandb->vigogne==0.2.0)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb->vigogne==0.2.0)\n","  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->vigogne==0.2.0) (67.7.2)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->vigogne==0.2.0) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->vigogne==0.2.0) (4.19.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->vigogne==0.2.0) (0.12.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->vigogne==0.2.0) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->vigogne==0.2.0) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->vigogne==0.2.0) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->vigogne==0.2.0) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->vigogne==0.2.0) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->vigogne==0.2.0) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->vigogne==0.2.0) (1.3.1)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->vigogne==0.2.0)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->vigogne==0.2.0) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->vigogne==0.2.0) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->vigogne==0.2.0) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->vigogne==0.2.0) (1.4.4)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->vigogne==0.2.0) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->vigogne==0.2.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->vigogne==0.2.0) (2023.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->vigogne==0.2.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->vigogne==0.2.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->vigogne==0.2.0) (2023.7.22)\n","Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->vigogne==0.2.0)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio->vigogne==0.2.0)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio->vigogne==0.2.0)\n","  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->vigogne==0.2.0) (1.3.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->vigogne==0.2.0) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->vigogne==0.2.0)\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->vigogne==0.2.0) (3.7.1)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->vigogne==0.2.0) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->vigogne==0.2.0) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->vigogne==0.2.0) (0.9.2)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio->vigogne==0.2.0) (1.1.3)\n","Building wheels for collected packages: vigogne, accelerate, fire, peft, rouge-score, transformers, ffmpy, pathtools\n","  Building wheel for vigogne (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for vigogne: filename=vigogne-0.2.0-py3-none-any.whl size=50546300 sha256=4fd89c9470830f94b3ca02906688e7d01d7dc4d8b77c01c05e2bf14557374847\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9vyd1b93/wheels/57/42/95/79f3f318720ec9843307cf4344b3987e92f9c4cbd22f95f324\n","  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for accelerate: filename=accelerate-0.23.0.dev0-py3-none-any.whl size=255448 sha256=18d59b108225cafb683012c5f638e06f51b3f02a1c8683d44513d98d1c67611b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9vyd1b93/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=b14943351016eccc0a79e0848d2c203eaebcd53e7d9acc84f497bb4afe68d695\n","  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for peft: filename=peft-0.6.0.dev0-py3-none-any.whl size=85089 sha256=cb7baa8e6999cbd2e6cb0b36aea7dbb0a71ec8fc11b805f1d5fe7f2ab3210bef\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9vyd1b93/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=61801db17035c6a5e58227e3547102fac67aaf9ce94ea9c737c93f84ade295ba\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.33.0.dev0-py3-none-any.whl size=7586983 sha256=1be72886549bd536dc037f39b0c8d232a76731d861e615ff5aa0544e3c71cd29\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9vyd1b93/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=9094e7e3c59e010ae0664f9e42222d78eb870ca53cd4ecd2cad260f00c0b6b84\n","  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=8e0a23087fc86c1f18695629666bc3ee27e2022bf194d83c9eb4d971f4161ada\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built vigogne accelerate fire peft rouge-score transformers ffmpy pathtools\n","Installing collected packages: tokenizers, sentencepiece, safetensors, pydub, pathtools, ffmpy, bitsandbytes, xxhash, websockets, tensorboardX, smmap, setproctitle, sentry-sdk, semantic-version, python-multipart, orjson, loralib, h11, fire, docker-pycreds, dill, aiofiles, uvicorn, tiktoken, starlette, rouge-score, multiprocess, huggingface-hub, httpcore, gitdb, transformers, openai, httpx, GitPython, fastapi, wandb, gradio-client, datasets, gradio, accelerate, peft, vigogne\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.22.0\n","    Uninstalling accelerate-0.22.0:\n","      Successfully uninstalled accelerate-0.22.0\n","Successfully installed GitPython-3.1.32 accelerate-0.23.0.dev0 aiofiles-23.2.1 bitsandbytes-0.41.1 datasets-2.14.4 dill-0.3.7 docker-pycreds-0.4.0 fastapi-0.102.0 ffmpy-0.3.1 fire-0.5.0 gitdb-4.0.10 gradio-3.41.2 gradio-client-0.5.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 loralib-0.1.1 multiprocess-0.70.15 openai-0.27.9 orjson-3.9.5 pathtools-0.1.2 peft-0.6.0.dev0 pydub-0.25.1 python-multipart-0.0.6 rouge-score-0.1.2 safetensors-0.3.3 semantic-version-2.10.0 sentencepiece-0.1.99 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 starlette-0.27.0 tensorboardX-2.6.2.2 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.33.0.dev0 uvicorn-0.23.2 vigogne-0.2.0 wandb-0.15.8 websockets-11.0.3 xxhash-3.3.0\n"]}]},{"cell_type":"markdown","source":["# Mount drive for models and persistence"],"metadata":{"id":"roNyHkF8wb0G"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_HNQd0eugY9","executionInfo":{"status":"ok","timestamp":1692995352417,"user_tz":-120,"elapsed":16503,"user":{"displayName":"Prudence Vol'Jin","userId":"11908822241771581805"}},"outputId":"1b57e513-a4f3-4fc8-e781-192088bfd42e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!ls -la /content/drive/MyDrive/models/vigogne2-7b-instruct/\n","!ls -la /content/drive/MyDrive/datasources/enno/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gfD7WWcuvOx","executionInfo":{"status":"ok","timestamp":1692995352948,"user_tz":-120,"elapsed":535,"user":{"displayName":"Prudence Vol'Jin","userId":"11908822241771581805"}},"outputId":"e804f38e-c8b1-4f5b-ebba-b7e6b7ac9f97"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["total 13161602\n","-rw------- 1 root root         21 Aug 18 09:02 added_tokens.json\n","-rw------- 1 root root        652 Aug 18 09:02 config.json\n","-rw------- 1 root root        175 Aug 18 09:02 generation_config.json\n","-rw------- 1 root root 1981889895 Aug 18 09:08 pytorch_model-00001-of-00007.bin\n","-rw------- 1 root root 1990296833 Aug 18 09:08 pytorch_model-00002-of-00007.bin\n","-rw------- 1 root root 1990296833 Aug 18 09:08 pytorch_model-00003-of-00007.bin\n","-rw------- 1 root root 1990296897 Aug 18 09:07 pytorch_model-00004-of-00007.bin\n","-rw------- 1 root root 1933656733 Aug 18 09:06 pytorch_model-00005-of-00007.bin\n","-rw------- 1 root root 1933673793 Aug 18 09:06 pytorch_model-00006-of-00007.bin\n","-rw------- 1 root root 1656836567 Aug 18 09:05 pytorch_model-00007-of-00007.bin\n","-rw------- 1 root root      26788 Aug 18 09:03 pytorch_model.bin.index.json\n","-rw------- 1 root root        414 Aug 18 09:03 special_tokens_map.json\n","-rw------- 1 root root        776 Aug 18 09:03 tokenizer_config.json\n","-rw------- 1 root root     499723 Aug 18 09:03 tokenizer.model\n","total 86\n","-rw------- 1 root root  1817 Jun  1 15:17 enno-faq-eval.jsonl\n","-rw------- 1 root root 85009 Aug 15 14:42 enno-faq-train.jsonl\n"]}]},{"cell_type":"markdown","source":["# Login wandb"],"metadata":{"id":"6kMfVZyBxuBJ"}},{"cell_type":"code","source":["!pip install wandb -qU\n","import wandb\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"id":"3xhRQulQxwPR","executionInfo":{"status":"ok","timestamp":1692995390604,"user_tz":-120,"elapsed":26492,"user":{"displayName":"Prudence Vol'Jin","userId":"11908822241771581805"}},"outputId":"a7a3a953-5ad3-4bf1-edfd-0f593eab8761"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"mwVt3Zqjwfwu"}},{"cell_type":"code","source":["!cd /content/vigogne && python vigogne/train/train_sft.py \\\n","--model_name_or_path \"/content/drive/MyDrive/models/vigogne2-7b-instruct/\" \\\n","--train_file \"/content/drive/MyDrive/datasources/enno//enno-faq-train.jsonl\" \\\n","--eval_file \"/content/drive/MyDrive/datasources/enno/enno-faq-eval.jsonl\" \\\n","--output_dir \"/content/drive/MyDrive/models/enno/outputs/vigogne2-7b-ft-enno-lora\" \\\n","--run_name vicuna-7b-ft-enno-lora \\\n","--overwrite_output_dir --mode instruct \\\n","--model_max_length 2048 \\\n","--preprocessing_num_workers 4 \\\n","--dataloader_num_workers 1 \\\n","--block_size 64 \\\n","--pack_into_block \\\n","--load_in_8bit \\\n","--lora_r 8 \\\n","--lora_alpha 16 \\\n","--lora_dropout 0.05 \\\n","--target_modules q_proj v_proj k_proj o_proj gate_proj down_proj up_proj \\\n","--per_device_train_batch_size 16 \\\n","--per_device_eval_batch_size 8 \\\n","--gradient_accumulation_steps 8 \\\n","--num_train_epochs 30 \\\n","--learning_rate 3e-4 \\\n","--warmup_ratio 0.05 \\\n","--weight_decay 0.01 \\\n","--gradient_checkpointing \\\n","--logging_steps 5 \\\n","--logging_first_step true \\\n","--save_strategy steps \\\n","--save_steps 5 \\\n","--save_total_limit 3 \\\n","--evaluation_strategy steps \\\n","--eval_steps 5 \\\n","--report_to wandb \\\n","--do_train \\\n","--do_eval \\\n","--compute_dtype bfloat16 \\\n","--load_best_model_at_end yes\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8GXbhltu5D8","executionInfo":{"status":"ok","timestamp":1692998927758,"user_tz":-120,"elapsed":3530374,"user":{"displayName":"Prudence Vol'Jin","userId":"11908822241771581805"}},"outputId":"4ea5361d-6fa6-402f-b222-5f59abc14841"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[2023-08-25 20:30:02,505] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","2023-08-25 20:30:09.137578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","08/25/2023 20:30:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n","Loading checkpoint shards: 100% 7/7 [03:12<00:00, 27.45s/it]\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","[ERROR|tokenization_utils_base.py:1061] 2023-08-25 20:33:25,071 >> Using pad_token, but it is not set yet.\n","[WARNING|modeling_utils.py:1505] 2023-08-25 20:33:25,072 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32002. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n","Downloading data files: 100% 1/1 [00:00<00:00, 2641.25it/s]\n","Extracting data files: 100% 1/1 [00:00<00:00,  5.36it/s]\n","Generating train split: 232 examples [00:00, 4597.21 examples/s]\n","Downloading data files: 100% 1/1 [00:00<00:00, 3785.47it/s]\n","Extracting data files: 100% 1/1 [00:00<00:00,  2.85it/s]\n","Generating train split: 7 examples [00:00, 2446.47 examples/s]\n","process dataset (num_proc=4):   0% 0/232 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2496] 2023-08-25 20:34:30,267 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","[WARNING|tokenization_utils_base.py:2496] 2023-08-25 20:34:30,275 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","[WARNING|tokenization_utils_base.py:2496] 2023-08-25 20:34:30,283 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","[WARNING|tokenization_utils_base.py:2496] 2023-08-25 20:34:30,316 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","process dataset (num_proc=4): 100% 232/232 [00:01<00:00, 191.17 examples/s]\n","process dataset (num_proc=4):   0% 0/7 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2496] 2023-08-25 20:34:31,792 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","[WARNING|tokenization_utils_base.py:2496] 2023-08-25 20:34:31,811 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","[WARNING|tokenization_utils_base.py:2496] 2023-08-25 20:34:31,838 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","[WARNING|tokenization_utils_base.py:2496] 2023-08-25 20:34:31,866 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","process dataset (num_proc=4): 100% 7/7 [00:00<00:00, 13.45 examples/s]\n","filter dataset by input length (num_proc=4): 100% 232/232 [00:00<00:00, 635.06 examples/s]\n","filter dataset by input length (num_proc=4): 100% 7/7 [00:00<00:00, 20.94 examples/s]\n","packing texts in blocks of 64: 100% 232/232 [00:00<00:00, 3828.23 examples/s]\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavid2-dutour\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.8\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/vigogne/wandb/run-20230825_203434-bgqo8603\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvicuna-7b-ft-enno-lora\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/david2-dutour/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/david2-dutour/huggingface/runs/bgqo8603\u001b[0m\n","  0% 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","{'loss': 2.0981, 'learning_rate': 0.0003, 'epoch': 0.53}\n","{'loss': 1.9018, 'learning_rate': 0.00023684210526315788, 'epoch': 2.67}\n"," 25% 5/20 [13:27<40:48, 163.26s/it]\n","                                   \n","\u001b[A{'eval_loss': 1.3103495836257935, 'eval_runtime': 1.2002, 'eval_samples_per_second': 5.832, 'eval_steps_per_second': 0.833, 'epoch': 2.67}\n"," 25% 5/20 [13:28<40:48, 163.26s/it]\n","100% 1/1 [00:00<00:00, 14.59it/s]\u001b[A\n","                                 \u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","{'loss': 1.5106, 'learning_rate': 0.00015789473684210524, 'epoch': 5.33}\n"," 50% 10/20 [27:02<27:26, 164.66s/it]\n","                                    \n","\u001b[A{'eval_loss': 0.9353552460670471, 'eval_runtime': 1.1176, 'eval_samples_per_second': 6.263, 'eval_steps_per_second': 0.895, 'epoch': 5.33}\n"," 50% 10/20 [27:03<27:26, 164.66s/it]\n","100% 1/1 [00:00<00:00, 13.53it/s]\u001b[A\n","                                 \u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","{'loss': 1.2819, 'learning_rate': 7.894736842105262e-05, 'epoch': 8.0}\n"," 75% 15/20 [40:25<13:20, 160.20s/it]\n","                                    \n","\u001b[A{'eval_loss': 0.7006350159645081, 'eval_runtime': 1.1102, 'eval_samples_per_second': 6.305, 'eval_steps_per_second': 0.901, 'epoch': 8.0}\n"," 75% 15/20 [40:26<13:20, 160.20s/it]\n","100% 1/1 [00:00<00:00, 12.72it/s]\u001b[A\n","                                 \u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","{'loss': 1.1382, 'learning_rate': 0.0, 'epoch': 10.67}\n","100% 20/20 [53:59<00:00, 164.57s/it]\n","                                    \n","\u001b[A{'eval_loss': 0.599619448184967, 'eval_runtime': 1.1153, 'eval_samples_per_second': 6.276, 'eval_steps_per_second': 0.897, 'epoch': 10.67}\n","100% 20/20 [54:00<00:00, 164.57s/it]\n","100% 1/1 [00:00<00:00, 13.07it/s]\u001b[A\n","{'train_runtime': 3246.6904, 'train_samples_per_second': 1.435, 'train_steps_per_second': 0.006, 'train_loss': 1.4679266810417175, 'epoch': 10.67}\n","100% 20/20 [54:05<00:00, 162.30s/it]\n","***** train metrics *****\n","  epoch                    =      10.67\n","  train_loss               =     1.4679\n","  train_runtime            = 0:54:06.69\n","  train_samples            =        233\n","  train_samples_per_second =      1.435\n","  train_steps_per_second   =      0.006\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","100% 1/1 [00:00<00:00, 14.95it/s]\n","***** eval metrics *****\n","  epoch                   =      10.67\n","  eval_loss               =     0.5996\n","  eval_runtime            = 0:00:01.17\n","  eval_samples            =          7\n","  eval_samples_per_second =      5.954\n","  eval_steps_per_second   =      0.851\n"]}]},{"cell_type":"markdown","source":["# Results"],"metadata":{"id":"F4O8fGdgFaBw"}},{"cell_type":"code","source":["!ls -la /content/drive/MyDrive/models/enno/outputs/vigogne2-7b-ft-enno-lora"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBsBc_gsFeZA","executionInfo":{"status":"ok","timestamp":1692982639438,"user_tz":-120,"elapsed":6,"user":{"displayName":"Prudence Vol'Jin","userId":"11908822241771581805"}},"outputId":"6995dfca-8439-4134-97eb-c58b754b17e8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["total 78246\n","-rw------- 1 root root      550 Aug 23 15:46 adapter_config.json\n","-rw------- 1 root root 80114765 Aug 23 15:46 adapter_model.bin\n","-rw------- 1 root root      351 Aug 23 15:46 all_results.json\n","drwx------ 2 root root     4096 Aug 23 15:32 checkpoint-10\n","-rw------- 1 root root      181 Aug 23 15:46 eval_results.json\n","-rw------- 1 root root     1485 Aug 23 15:46 README.md\n","-rw------- 1 root root      190 Aug 23 15:46 train_results.json\n"]}]}]}