<p align="center" width="100%">
<img src="./assets/vigogne_logo.png" alt="Vigogne" style="width: 40%; min-width: 300px; display: block; margin: auto;">
</p>

# Vigogne ü¶ô: French Instruction-following and Chat Models

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/bofenghuang/vigogne/blob/main/LICENSE)
[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/bofenghuang/vigogne/blob/main/DATA_LICENSE)
[![Models](https://img.shields.io/badge/Hugging%20Face-Models-yellow.svg)](https://huggingface.co/models?search=bofenghuang/vigogne)

*The vigogne (French name for vicu√±a) is a South American camelid native to the Andes Mountains. It is closely related to the llama, alpaca, and guanaco.*

Vigogne is a collection of powerful French üá´üá∑ large language models (LLMs) that are open-source and designed for instruction-following and chat purposes.

The main contributions of this project include:

- Open-sourced ü¶ô Vigogne models for French instruction-following and chat
- Efficient training code for fine-tuning LLMs such as [LLaMA](https://github.com/facebookresearch/llama), [Llama-2](https://ai.meta.com/llama), [Falcon](https://falconllm.tii.ae), and [FLAN-T5](https://huggingface.co/google/flan-t5-xl)
- Generated, translated, and collected French instruction-following and dialogue datasets, along with the used scripts
- Inference code and gradio demo, as well as detailed instructions for experiencing the quantized Vigogne models on your PC



üí° *The screencast below shows the current ü¶ô Vigogne-7B-Chat model running on Apple M1 Pro using 4GB of weights (no sped up).*

![](./assets/screencast_llamacpp_chat.gif)

## Table of Contents

- [Updates](#updates)
- [Installation](#installation)
- [ü¶ô Vigogne Models](#-vigogne-models)
- [Inference and Deployment](#inference-and-deployment)
- [Data](#data)
- [Training](#training)
- [Example Outputs](#example-outputs)
- [Bias, Risks, and Limitations](#bias-risks-and-limitations)
- [Acknowledgements](#acknowledgements)
- [Citation](#citation)

## Updates

- [2023/07/20]: Introduce the latest Vigogne models, based on Llama-2.
- [2023/07/05]: Released Vigogne models derived from Falcon and MPT, offering a more friendly license for commercial use.
- [2023/06/05]: Provided support for [QLoRA](https://arxiv.org/abs/2305.14314), resulting in a significant memory usage reduction.
- [2023/05/15]: Unveiled the Vigogne-Chat models and integrated the Gradio Demo for interactive chat demonstrations.
- [2023/05/11]: Added support for DeepSpeed, significantly improving the efficiency of model training.
- [2023/05/11]: Implemented chat data generation using [Self-Chat](https://arxiv.org/abs/2304.01196), empowering the model with more conversational abilities.
- [2023/05/11]: Presented the upgraded Vigogne-Instruct V2 models, trained on a larger and more diverse dataset.
- [2023/05/11]: Released annotated French seed tasks and the generation script for Self-Instruct.
- [2023/04/03]: Provided a training script for seq2seq models, utilizing LoRA and LLM.int8().
- [2023/03/29]: Included deployment instructions using [llama.cpp](https://github.com/ggerganov/llama.cpp).
- [2023/03/26]: Open-sourced the Vigogne projet and released fine-tuned LLaMA and BLOOM models trained on the translated Stanford Alpaca dataset.

## Installation

1. Clone this repository

```bash
git clone https://github.com/bofenghuang/vigogne.git
cd vigogne
```

2. Install the package

```bash
# Install DeepSpeed if want to accelerate training with it
pip install deepspeed

pip install .
```

## ü¶ô Vigogne Models

The fine-tuned ü¶ô Vigogne models come in two types: **instruction-following models** and **chat models**. The instruction-following models are optimized to generate concise and helpful responses to user instructions, similar to `text-davinci-003`. Meanwhile, the chat models are designed for multi-turn dialogues, but they also perform well in instruction-following tasks, similar to `gpt-3.5-turbo`.

More information can be found in the [vigogne/model](docs/model.md).

## Inference and Deployment

This repository offers multiple options for inference and deployment, such as a **Google Colab notebook**, **Gradio demo**, and instructions for running experiments on your own PC using [**llama.cpp**](https://github.com/ggerganov/llama.cpp).

More information can be found in the [vigogne/inference](docs/inference.md).

## Data

The Vigogne models were trained using open-source datasets, ChatGPT-distillation datasets (self-instruct, self-chatting), and translated datasets.

More information can be found in the [vigogne/data](docs/data.md).

## Training

To fine-tune LLMs more efficiently, we employ a technique called [low-rank adaptation (LoRA)](https://arxiv.org/abs/2106.09685) provided by ü§ó Hugging Face's [PEFT](https://github.com/huggingface/peft) library, which involves freezing the base model's weights and adding a small number of learnable parameters.

In addition, you can further reduce the memory usage during fine-tuning by using [LLM.int8()](https://arxiv.org/abs/2208.07339), which employs a 2-stage quantization method that quantizes part of the computation to int8. This enables efficient training on a single consumer GPU such as the RTX 4090. However, it may be slightly slower than the fp16 version. If your GPUs have enough memory, you can skip this step and train using [DeepSpeed](https://github.com/microsoft/DeepSpeed).

More information can be found in the [vigogne/training](docs/training.md).

## Example Outputs

R√©pondez √† la question suivante : Les pratiques artistiques transforment-elles le monde ?

- **vigogne-7b-instruct**: Les pratiques d'art peuvent transformer le monde en cr√©ant des ≈ìuvres qui √©voquent des sentiments, des id√©es et des √©motions. Elles peuvent √©galement √™tre utilis√©es pour cr√©er des ≈íuvres d'Art Vivant qui sont des ≈ìuvre vivantes qui peuvent √™tre utilis√©s pour cr√©√© des exp√©riences et des conversations. Les praticiens d'arts peuvent √©galement utiliser leurs pratiques pour cr√©erer des ≈ìufs d'√¢me, des √™tres vivants cr√©√©s √† partir de mat√©riaux naturels, qui peuvent avoir un impact sur la fa√ßon dont les gens pensent et ressentent le monde.

<details><summary>Other models</summary>

- **Alpaca-LoRA**: Yes, artistic practices can transform the world. Art can be used to express ideas, explore new perspectives, and create dialogue. Art can also be used to inspire and motivate people to take action, and to create positive change in their communities.

- **text-davinci-003**: Oui, les pratiques artistiques peuvent transformer le monde. Les artistes peuvent utiliser leur art pour exprimer leurs opinions et leurs id√©es, et ces id√©es peuvent avoir un impact sur la soci√©t√© et le monde. Les artistes peuvent √©galement utiliser leur art pour sensibiliser le public √† des sujets importants et pour encourager le changement. Les pratiques artistiques peuvent donc √™tre un puissant outil pour transformer le monde.

</details>

---

Expliquez la diff√©rence entre DoS et phishing en fran√ßais.

- **vigogne-7b-instruct**: Le DoS (Denial of Service) est une attaque informatique qui consiste √† bloquer ou √† interrompre l'acc√®s √† un service ou √† un syst√®me informatique. Le phishing est une technique de fraude qui consisterait √† obtenir des informations sensibles d'un utilisateur en lui demandant de fournir des renseignements, tels que des mots de passe ou des num√©ros de carte de cr√©dit.

<details><summary>Other models</summary>

- **Alpaca-LoRA**: In French, the difference between a DDoS attack and phishing is that a DDoS attack is an attempt to overwhelm a system with a large amount of traffic, while phishing is an attempt to steal personal information by posing as a trusted source.

- **text-davinci-003**:

DoS (Denial of Service) est une attaque qui vise √† rendre indisponible des services web ou r√©seau en saturant un serveur ou un r√©seau avec des requ√™tes. Elle peut aussi √™tre utilis√©e pour perturber un r√©seau en envoyant des paquets mal form√©s ou en le surchargant.

Phishing est une forme d'escroquerie en ligne visant √† voler des informations personnelles telles que des mots de passe et des num√©ros de carte de cr√©dit en incitant les victimes √† fournir leurs informations sur des faux sites Web ou des emails frauduleux.

</details>

---

R√©sumez le texte suivant en une seule phrase.

La science des donn√©es est l'√©tude des donn√©es afin d'en extraire des informations significatives pour les entreprises. Il s'agit d'une approche pluridisciplinaire qui combine des principes et des pratiques issus des domaines des math√©matiques, des statistiques, de l'intelligence artificielle et du g√©nie informatique, en vue d'analyser de grands volumes de donn√©es. Cette analyse aide les scientifiques des donn√©es √† poser des questions et √† y r√©pondre, comme Que s'est-il pass√©, Pourquoi cela s'est-il pass√©, Que va-t-il se passer et Que peut-on faire avec des r√©sultats.

- **vigogne-7b-instruct**: Le scientifique des donn√©es utilise des donn√©es pour analyser et extraire de l‚Äôinformation pour aider les entrepreneurs √† posent des questions, r√©pondre √† des questions existantes et prendre des d√©cisions.

<details><summary>Other models</summary>

- **Alpaca-LoRA**: La science des donn√©es est l'√©tude des donn√©es afin d'en extraire des informations significatives pour les entreprises.

- **text-davinci-003**: La science des donn√©es combine des principes et des pratiques issus des domaines des math√©matiques, des statistiques, de l'intelligence artificielle et du g√©nie informatique pour analyser de grands volumes de donn√©es afin d'en extraire des informations significatives pour les entreprises.

</details>

---

√âcrivez une fonction qui prend une liste de cha√Ænes de caract√®res et renvoie une liste sans doublons.

- **vigogne-7b-instruct**:

```python
def remove_duplicates(lst):
    result = []
    for item in lst:
        if item not in result:
            result.append(item)
    return result
```

<details><summary>Other models</summary>

- **Alpaca-LoRA**:

```python
def remove_duplicates(list):
    seen = set()
    return [x for x in list if x not in seen]
```

- **text-davinci-003**:

```python
def remove_duplicates(liste):
  new_list = []
  for item in liste:
    if item not in new_list:
      new_list.append(item)
  return new_list

liste = ["a", "b", "c", "a", "d", "e", "f", "c"]
print(remove_duplicates(liste)) # ['a', 'b', 'c', 'd', 'e', 'f']
```

</details>

## Bias, Risks, and Limitations

Vigogne is still under development, and there are many limitations that have to be addressed. Please note that it is possible that the model generates harmful or biased content, incorrect information or generally unhelpful answers.

## Acknowledgements

Our project builds upon the following open-source projects for further development. We would like to extend our sincerest gratitude to the individuals involved in the research and development of these projects.

- [ü§ó Transformers](https://github.com/huggingface/transformers) and [ü§ó PEFT](https://github.com/huggingface/peft)
- [LLaMA](https://github.com/facebookresearch/llama)
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
- [Alpaca-LoRA by @tloen](https://github.com/tloen/alpaca-lora)
- [Baize](https://github.com/project-baize/baize-chatbot)
- [llama.cpp by @ggerganov](https://github.com/ggerganov/llama.cpp)

## Citation

If you find the model, data, and code in our project useful, please consider citing our work as follows:

```
@misc{vigogne,
  author = {Bofeng Huang},
  title = {Vigogne: French Instruction-following and Chat Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/bofenghuang/vigogne}},
}
```
